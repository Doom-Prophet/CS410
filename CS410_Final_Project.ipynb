{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSKMlE8aHriTpWrievPoF3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Doom-Prophet/CS410/blob/main/CS410_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS410 Final Project\n",
        "**Name: Zicheng Ma**\n",
        "\n",
        "**NetID: zicheng5**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K-5Vpqvau_B9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "Automatic document summarization is a vital area in natural language processing and information retrieval. The abundance of digital textual data necessitates efficient methods to condense information while preserving its essential meaning. This project aims to create a summarization system that employs machine learning techniques to generate succinct and informative summaries\n",
        "from input documents."
      ],
      "metadata": {
        "id": "WGYMZP5MwM-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "BO5P1ixIzYnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMie4Tmi2Amh",
        "outputId": "1f9421b7-aabc-48e3-f39c-577796bd6cbb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if not os.path.exists(\"/content/gdrive/My Drive/Colab Notebooks/CS410_Final_Project\"):\n",
        "    os.makedirs(\"/content/gdrive/My Drive/Colab Notebooks/CS410_Final_Project\")\n",
        "os.chdir(\"/content/gdrive/My Drive/Colab Notebooks/CS410_Final_Project\")"
      ],
      "metadata": {
        "id": "BkiOVz4N4MIE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd # Check if this is CS410 Final Project folder\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GluRa-uV4Urn",
        "outputId": "0b964d60-fed0-4f99-89f0-473559690239"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/CS410_Final_Project\n",
            "datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install nltk\n",
        "!pip install beautifulsoup4\n",
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TzHMBOdzXrf",
        "outputId": "ff15bc55-ccbf-4655-fd2c-34e53a0bfc2e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection and Preprocessing\n",
        "Hugging Face’s datasets library provides an easy way to download and load the CNN/Daily Mail dataset."
      ],
      "metadata": {
        "id": "J1PbtjJuwaOZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CMhYDZy5u6tc"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_dir = \"/content/gdrive/My Drive/Colab Notebooks/CS410_Final_Project/datasets/cnn_dailymail\"\n",
        "\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", cache_dir=dataset_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define functions to clean and preprocess the text."
      ],
      "metadata": {
        "id": "bWJ9QQsZzj-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = clean_html(text)\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
        "    text = text.strip().lower()  # Convert to lowercase and remove leading/trailing whitespace\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]  # Remove non-alphabetic words and stopwords\n",
        "    return ' '.join(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELIh9DXHzmfb",
        "outputId": "13653262-b1d0-4d56-ab45-acd3ba61cc82"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply the preprocessing function to a subset of the dataset."
      ],
      "metadata": {
        "id": "ErcKBoWlzpZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing to a subset of the dataset (e.g., first 1000 articles for now)\n",
        "num_samples = 1000\n",
        "preprocessed_articles = [preprocess_text(article) for article in dataset['train']['article'][:num_samples]]\n",
        "preprocessed_summaries = [preprocess_text(summary) for summary in dataset['train']['highlights'][:num_samples]]\n",
        "\n",
        "preprocessed_articles_for_testing = [preprocess_text(article) for article in dataset['train']['article'][num_samples+1:2*num_samples]]\n",
        "preprocessed_summaries_for_testing = [preprocess_text(summary) for summary in dataset['train']['highlights'][num_samples+1:2*num_samples]]\n",
        "\n",
        "# Show the first preprocessed article and summary\n",
        "print(\"Article:\", preprocessed_articles[0])\n",
        "print(\"Summary:\", preprocessed_summaries[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQnIfNGLzrtD",
        "outputId": "bd7b6605-e537-49a9-dfd9-3420fd3e3c71"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-ad23b5349bbf>:14: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article: london england reuters harry potter star daniel radcliffe gains access reported million million fortune turns monday insists money wo cast spell daniel radcliffe harry potter harry potter order phoenix disappointment gossip columnists around world young actor says plans fritter cash away fast cars drink celebrity parties plan one people soon turn suddenly buy massive sports car collection something similar told australian interviewer earlier month think particularly extravagant things like buying things cost pounds books cds dvds radcliffe able gamble casino buy drink pub see horror film hostel part ii currently six places number one movie uk box office chart details mark landmark birthday wraps agent publicist comment plans definitely sort party said interview hopefully none reading radcliffe earnings first five potter films held trust fund able touch despite growing fame riches actor says keeping feet firmly ground people always looking say star goes rails told reporters last month try hard go way would easy latest outing boy wizard harry potter order phoenix breaking records sides atlantic reprise role last two films watch give review potter latest life beyond potter however londoner filmed tv movie called boy jack author rudyard kipling son due release later year also appear december boys australian film four boys escape orphanage earlier year made stage debut playing tortured teenager peter shaffer equus meanwhile braced even closer media scrutiny legally adult think going sort fair game told reuters friend copyright reuters rights material may published broadcast rewritten redistributed\n",
            "Summary: harry potter star daniel radcliffe gets fortune turns monday young actor says plans fritter cash away radcliffe earnings first five potter films held trust fund\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show Evidence of Adequacy"
      ],
      "metadata": {
        "id": "lCe5-8b0zut5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_statistics(tokenized_documents):\n",
        "    total_docs = len(tokenized_documents)\n",
        "    total_words = sum(len(doc.split()) for doc in tokenized_documents)\n",
        "    vocabulary = set(word for doc in tokenized_documents for word in doc.split())\n",
        "\n",
        "    print(\"Total Documents:\", total_docs)\n",
        "    print(\"Total Words:\", total_words)\n",
        "    print(\"Average Document Length:\", total_words / total_docs)\n",
        "    print(\"Vocabulary Size:\", len(vocabulary))\n",
        "\n",
        "dataset_statistics(preprocessed_articles)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Awwc88Izwwt",
        "outputId": "eb257a4a-2677-4c09-d090-46f3ef0d671d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Documents: 1000\n",
            "Total Words: 321810\n",
            "Average Document Length: 321.81\n",
            "Vocabulary Size: 26482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Analysis Techniques\n",
        "Exploration and implementation of text analysis techniques such as word association, topic modeling (e.g., Latent Dirichlet Allocation), text clustering/categorization, and semantic analysis (e.g., Word2Vec, BERT embeddings) to identify significant content and relationships within the documents."
      ],
      "metadata": {
        "id": "HXhZTLGfweo6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Word Association**\n"
      ],
      "metadata": {
        "id": "eadEiqBZJ-Pb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NcYjMYIcKgPK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Topic Modeling**"
      ],
      "metadata": {
        "id": "4by73fEaKgkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# `preprocessed_articles` is list of preprocessed text documents\n",
        "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
        "X = vectorizer.fit_transform(preprocessed_articles)\n",
        "\n",
        "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "lda.fit(X)\n",
        "\n",
        "# Display topics\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "for topic_idx, topic in enumerate(lda.components_):\n",
        "    print(f\"Topic #{topic_idx + 1}:\")\n",
        "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]))\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eq56WMtIKi2y",
        "outputId": "1735e2c1-65ae-4538-ec2d-00241a0f5899"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic #1:\n",
            "said obama clinton cnn new mccain watch campaign people percent\n",
            "\n",
            "Topic #2:\n",
            "said new company world think oil says percent years people\n",
            "\n",
            "Topic #3:\n",
            "said iraq cnn government people military president united country security\n",
            "\n",
            "Topic #4:\n",
            "said cnn children like people family years time says bush\n",
            "\n",
            "Topic #5:\n",
            "said police cnn told home time friend people says family\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Text Clustering/Categorization**"
      ],
      "metadata": {
        "id": "j0Dcp5iDhAtF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Id2hEJWnhBaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Semantic Analysis**"
      ],
      "metadata": {
        "id": "qidZ27r9hBfO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EYXwdFSRhBo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarization Model\n"
      ],
      "metadata": {
        "id": "3qp5WgjFwffk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Define the Model**\n",
        "\n",
        "Use transformer-based models T5, which has been shown to perform well on summarization tasks.\n",
        "\n",
        "Use the Hugging Face Transformers library to easily load a pre-trained model and fine-tune it on my dataset."
      ],
      "metadata": {
        "id": "c_uVE444Lkjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "class TopicEmbeddingModel(nn.Module):\n",
        "    def __init__(self, model, num_topics):\n",
        "        super(TopicEmbeddingModel, self).__init__()\n",
        "        self.model = model\n",
        "        self.topic_embedding = nn.Linear(num_topics, model.config.d_model)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, topic_distribution):\n",
        "        # Convert topic distribution to tensor\n",
        "        topic_tensor = torch.tensor(topic_distribution).float().to(input_ids.device)\n",
        "        if len(topic_tensor.size()) == 1:\n",
        "            topic_tensor = topic_tensor.unsqueeze(0)\n",
        "\n",
        "        # Transform topic tensor dimensions\n",
        "        topic_tensor = self.topic_embedding(topic_tensor)\n",
        "\n",
        "        # Get input embeddings\n",
        "        input_embeddings = self.model.get_input_embeddings()(input_ids)\n",
        "\n",
        "        # Add topic information to embeddings\n",
        "        extended_embeddings = input_embeddings + topic_tensor.unsqueeze(1)\n",
        "\n",
        "        # Forward pass through the model\n",
        "        outputs = self.model.encoder(inputs_embeds=extended_embeddings, attention_mask=attention_mask)\n",
        "        return outputs\n",
        "\n"
      ],
      "metadata": {
        "id": "NfLEJRvQwflq"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Integrate Topic Distributions**\n",
        "\n",
        "Modify the model to accept the topic distribution as an additional input. Here I concatenate the topic distribution to the token embeddings of the input document."
      ],
      "metadata": {
        "id": "yQqiVHjhLrxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "num_topics = 5\n",
        "topic_model = TopicEmbeddingModel(model, num_topics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bO_tyJmLsHQ",
        "outputId": "a222c1bf-eb55-4c05-de84-4263c89b5554"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Fine-Tune the Model**\n",
        "\n",
        "Fine-tune the model on the dataset."
      ],
      "metadata": {
        "id": "7mkbj4omLsQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AdamW\n",
        "\n",
        "# optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     for batch in dataloader:\n",
        "#         # Unpack the batch\n",
        "#         input_ids, attention_mask, labels, topic_distributions = batch\n",
        "#\n",
        "#         # Modify the embeddings\n",
        "#         extended_embeddings = add_topic_information(input_ids, topic_distributions)\n",
        "#\n",
        "#         # Forward pass\n",
        "#         outputs = model(inputs_embeds=extended_embeddings, attention_mask=attention_mask, labels=labels)\n",
        "#\n",
        "#         # Compute loss\n",
        "#         loss = outputs.loss\n",
        "#\n",
        "#         # Backward pass\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         optimizer.zero_grad()\n"
      ],
      "metadata": {
        "id": "s8fIcHhLLsVl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Generate Summaries**"
      ],
      "metadata": {
        "id": "bpIhdqQ9LsZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_text = preprocessed_articles_for_testing[0]\n",
        "print(\"Article:\", document_text)\n",
        "print(\"Standard Summary:\", preprocessed_summaries_for_testing[0])\n",
        "\n",
        "inputs = tokenizer(document_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "\n",
        "topic_distribution = [0.1, 0.3, 0.2, 0.25, 0.15]\n",
        "\n",
        "# Forward pass through the topic model\n",
        "encoder_outputs = topic_model(inputs['input_ids'], inputs['attention_mask'], topic_distribution)\n",
        "\n",
        "# Generate summary IDs\n",
        "summary_ids = model.generate(encoder_outputs=encoder_outputs, max_length=150, attention_mask=inputs['attention_mask'])\n",
        "\n",
        "# Decode summary IDs to text\n",
        "summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "print(\"Generated Summary:\", summary_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOKvh4rBLscT",
        "outputId": "b7db081c-4650-42ff-ca0f-cae5b204ab6b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article: cnn bus carrying high school band students tipped saturday interstate northwest minneapolis minnesota killing one person bus carrying school band members rests upright crashed saturday minnesota three people critically injured authorities said second bus traveling one crashed affected according report posted web site pelican rapids school district students pelican rapids high school returning band trip chicago illinois accident happened near albertville minnesota minnesota highway patrol said people including driver westbound bus tipped minnesota highway patrol said everyone bus taken hospitals treatment evaluation school district said watch rescuers work scene pelican rapids minnesota cause accident investigated friend\n",
            "Standard Summary: bus carrying high school students tips minnesota interstate one person killed three critically injured authorities say two buses pelican rapids minnesota way home chicago illinois\n",
            "Generated Summary: ota highway patrol says buses taken hospitals treatment evaluation school district says watch rescuers work scene pelican rapids minnesota cause accident investigated friend. bus carrying high school band members rests upright crashed saturday minnesota three people critically injured authorities said second bus traveling one crashed affected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "The system’s performance will be evaluated using standard metrics for summarization tasks, including ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metrics, precision, recall, and F1-score."
      ],
      "metadata": {
        "id": "-i19fmKIwfpv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1MFpfWPuwftz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}